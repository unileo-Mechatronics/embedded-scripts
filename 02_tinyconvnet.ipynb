{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d4dcfe64",
      "metadata": {
        "id": "d4dcfe64"
      },
      "source": [
        "# 02 â€“ TinyConvNet\n",
        "Implement `TinyConvNet` and experiment with il training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc59a508",
      "metadata": {
        "id": "bc59a508"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"Train samples:\", len(train_dataset))\n",
        "print(\"Test samples:\", len(test_dataset))\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # [B,6,14,14]\n",
        "        x = self.pool(self.relu(self.conv2(x)))  # [B,16,5,5]\n",
        "        x = x.view(x.size(0), -1)                # [B,400]\n",
        "        x = self.relu(self.fc1(x))               # [B,120]\n",
        "        x = self.relu(self.fc2(x))               # [B,84]\n",
        "        x = self.fc3(x)                          # [B,10]\n",
        "        return x\n",
        "\n",
        "# TODO: Implement TinyConvNet\n",
        "#   - Conv2d(1 -> 4, kernel_size=3, padding=1) + ReLU + MaxPool2d(2)\n",
        "#   - Conv2d(4 -> 8, kernel_size=3, padding=1) + ReLU + MaxPool2d(2)\n",
        "#   - Global Average Pooling (AdaptiveAvgPool2d((1,1)))\n",
        "#   - Linear(8 -> 10)\n",
        "\n",
        "class TinyConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyConvNet, self).__init__()\n",
        "        # TODO: definire gli strati\n",
        "        # self.conv1 = ...\n",
        "        # self.conv2 = ...\n",
        "        # self.pool = ...\n",
        "        # self.relu = ...\n",
        "        # self.gap = ...\n",
        "        # self.fc = ...\n",
        "        raise NotImplementedError(\"TODO: implement TinyConvNet constructor\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: definire la forward\n",
        "        # 1) Conv1 -> ReLU -> pool\n",
        "        # 2) Conv2 -> ReLU -> pool\n",
        "        # 3) Global Average Pooling\n",
        "        # 4) Flatten\n",
        "        # 5) Fully connected finale\n",
        "        raise NotImplementedError(\"TODO: Implement TinyConvNet forward\")\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# TODO: Chose the model to train\n",
        "MODEL_NAME = \"lenet\"\n",
        "\n",
        "if MODEL_NAME == \"lenet\":\n",
        "    model = LeNet().to(device)\n",
        "elif MODEL_NAME == \"tiny\":\n",
        "    model = TinyConvNet().to(device)\n",
        "else:\n",
        "    raise ValueError(\"MODEL_NAME deve essere 'lenet' o 'tiny'.\")\n",
        "\n",
        "print(f\"\\n>>> Modello selezionato: {MODEL_NAME}\")\n",
        "print(model)\n",
        "print(\"Numero parametri:\", count_parameters(model))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 3  # TODO: Try to change epochs number\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "        f\"- Train loss: {train_loss:.4f}, acc: {train_acc*100:.2f}% \"\n",
        "        f\"- Test loss: {test_loss:.4f}, acc: {test_acc*100:.2f}%\"\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
